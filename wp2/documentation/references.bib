
@article{jooken_exploring_2023,
	title = {Exploring search space trees using an adapted version of {Monte} {Carlo} tree search for combinatorial optimization problems},
	volume = {150},
	issn = {03050548},
	url = {http://arxiv.org/abs/2010.11523},
	doi = {10.1016/j.cor.2022.106070},
	abstract = {In this article we propose a heuristic algorithm to explore search space trees associated with instances of combinatorial optimization problems. The algorithm is based on Monte Carlo tree search, a popular algorithm in game playing that is used to explore game trees and represents the state-of-the-art algorithm for a number of games. Several enhancements to Monte Carlo tree search are proposed that make the algorithm more suitable in a combinatorial optimization context. These enhancements exploit the combinatorial structure of the problem and aim to efﬁciently explore the search space tree by pruning subtrees, using a heuristic simulation policy, reducing the domains of variables by eliminating dominated value assignments and using a beam width. The algorithm was implemented with its components specifically tailored to two combinatorial optimization problems: the quay crane scheduling problem with non-crossing constraints and the 0-1 knapsack problem. For the ﬁrst problem our algorithm surpasses the state-of-the-art results and several new best solutions are found for a benchmark set of instances. For the second problem our algorithm typically produces near-optimal solutions that are slightly worse than the state-of-the-art results, but it needs only a small fraction of the time to do so. These results indicate that the algorithm is competitive with the state-of-the-art for two entirely different combinatorial optimization problems.},
	language = {en},
	urldate = {2025-08-28},
	journal = {Computers \& Operations Research},
	author = {Jooken, Jorik and Leyman, Pieter and Wauters, Tony and Causmaecker, Patrick De},
	month = feb,
	year = {2023},
	note = {arXiv:2010.11523 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Mathematics - Optimization and Control, Computer Science - Discrete Mathematics},
	pages = {106070},
	file = {PDF:C\:\\Users\\mboegl\\Zotero\\storage\\NQDVQB6K\\Jooken et al. - 2023 - Exploring search space trees using an adapted version of Monte Carlo tree search for combinatorial o.pdf:application/pdf},
}

@misc{orseau_single-agent_2018,
	title = {Single-{Agent} {Policy} {Tree} {Search} {With} {Guarantees}},
	url = {http://arxiv.org/abs/1811.10928},
	doi = {10.48550/arXiv.1811.10928},
	abstract = {We introduce two novel tree search algorithms that use a policy to guide search. The first algorithm is a best-first enumeration that uses a cost function that allows us to prove an upper bound on the number of nodes to be expanded before reaching a goal state. We show that this best-first algorithm is particularly well suited for `needle-in-a-haystack' problems. The second algorithm is based on sampling and we prove an upper bound on the expected number of nodes it expands before reaching a set of goal states. We show that this algorithm is better suited for problems where many paths lead to a goal. We validate these tree search algorithms on 1,000 computer-generated levels of Sokoban, where the policy used to guide the search comes from a neural network trained using A3C. Our results show that the policy tree search algorithms we introduce are competitive with a state-of-the-art domain-independent planner that uses heuristic search.},
	urldate = {2025-10-17},
	publisher = {arXiv},
	author = {Orseau, Laurent and Lelis, Levi H. S. and Lattimore, Tor and Weber, Théophane},
	month = nov,
	year = {2018},
	note = {arXiv:1811.10928 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\mboegl\\Zotero\\storage\\IBNAEZUE\\Orseau et al. - 2018 - Single-Agent Policy Tree Search With Guarantees.pdf:application/pdf;Snapshot:C\:\\Users\\mboegl\\Zotero\\storage\\R2GGSESZ\\1811.html:text/html},
}

@article{kemmerling_beyond_2024,
	title = {Beyond games: a systematic review of neural {Monte} {Carlo} tree search applications},
	volume = {54},
	issn = {0924-669X, 1573-7497},
	shorttitle = {Beyond games},
	url = {https://link.springer.com/10.1007/s10489-023-05240-w},
	doi = {10.1007/s10489-023-05240-w},
	abstract = {The advent of AlphaGo and its successors marked the beginning of a new paradigm in playing games using artiﬁcial intelligence. This was achieved by combining Monte Carlo tree search, a planning procedure, and deep learning. While the impact on the domain of games has been undeniable, it is less clear how useful similar approaches are in applications beyond games and how they need to be adapted from the original methodology. We perform a systematic literature review of peer-reviewed articles detailing the application of neural Monte Carlo tree search methods in domains other than games. Our goal is to systematically assess how such methods are structured in practice and if their success can be extended to other domains. We ﬁnd applications in a variety of domains, many distinct ways of guiding the tree search using learned policy and value functions, and various training methods. Our review maps the current landscape of algorithms in the family of neural monte carlo tree search as they are applied to practical problems, which is a ﬁrst step towards a more principled way of designing such algorithms for speciﬁc problems and their requirements.},
	language = {en},
	number = {1},
	urldate = {2025-10-20},
	journal = {Applied Intelligence},
	author = {Kemmerling, Marco and Lütticke, Daniel and Schmitt, Robert H.},
	month = jan,
	year = {2024},
	pages = {1020--1046},
	file = {PDF:C\:\\Users\\mboegl\\Zotero\\storage\\8K99WSH3\\Kemmerling et al. - 2024 - Beyond games a systematic review of neural Monte Carlo tree search applications.pdf:application/pdf},
}

@misc{xing_solve_2020,
	title = {Solve {Traveling} {Salesman} {Problem} by {Monte} {Carlo} {Tree} {Search} and {Deep} {Neural} {Network}},
	url = {http://arxiv.org/abs/2005.06879},
	doi = {10.48550/arXiv.2005.06879},
	abstract = {We present a self-learning approach that combines deep reinforcement learning and Monte Carlo tree search to solve the travelling salesman problem. The proposed approach has two advantages. First, it adopts deep reinforcement learning to compute the value functions for decision, which removes the need of hand-crafted features and labelled data. Second, it uses Monte Carlo tree search to select the best policy by comparing different value functions, which increases its generalization ability. Experimental results show that the proposed method performs favorably against other methods in small-to-medium problem settings. And it shows comparable performance as state-of-the-art in large problem setting.},
	language = {en},
	urldate = {2025-10-23},
	publisher = {arXiv},
	author = {Xing, Zhihao and Tu, Shikui and Xu, Lei},
	month = may,
	year = {2020},
	note = {arXiv:2005.06879 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {PDF:C\:\\Users\\mboegl\\Zotero\\storage\\HMT86P53\\Xing et al. - 2020 - Solve Traveling Salesman Problem by Monte Carlo Tree Search and Deep Neural Network.pdf:application/pdf},
}

@misc{oren_solo_2021,
	title = {{SOLO}: {Search} {Online}, {Learn} {Offline} for {Combinatorial} {Optimization} {Problems}},
	shorttitle = {{SOLO}},
	url = {http://arxiv.org/abs/2104.01646},
	doi = {10.48550/arXiv.2104.01646},
	abstract = {We study combinatorial problems with real world applications such as machine scheduling, routing, and assignment. We propose a method that combines Reinforcement Learning (RL) and planning. This method can equally be applied to both the ofﬂine, as well as online, variants of the combinatorial problem, in which the problem components (e.g., jobs in scheduling problems) are not known in advance, but rather arrive during the decision-making process. Our solution is quite generic, scalable, and leverages distributional knowledge of the problem parameters. We frame the solution process as an MDP, and take a Deep Q-Learning approach wherein states are represented as graphs, thereby allowing our trained policies to deal with arbitrary changes in a principled manner. Though learned policies work well in expectation, small deviations can have substantial negative effects in combinatorial settings. We mitigate these drawbacks by employing our graph-convolutional policies as non-optimal heuristics in a compatible search algorithm, Monte Carlo Tree Search, to signiﬁcantly improve overall performance. We demonstrate our method on two problems: Machine Scheduling and Capacitated Vehicle Routing. We show that our method outperforms custom-tailored mathematical solvers, state of the art learning-based algorithms, and common heuristics, both in computation time and performance.},
	language = {en},
	urldate = {2025-10-23},
	publisher = {arXiv},
	author = {Oren, Joel and Ross, Chana and Lefarov, Maksym and Richter, Felix and Taitler, Ayal and Feldman, Zohar and Daniel, Christian and Castro, Dotan Di},
	month = may,
	year = {2021},
	note = {arXiv:2104.01646 [cs]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {PDF:C\:\\Users\\mboegl\\Zotero\\storage\\AQE6MEAP\\Oren et al. - 2021 - SOLO Search Online, Learn Offline for Combinatorial Optimization Problems.pdf:application/pdf},
}
